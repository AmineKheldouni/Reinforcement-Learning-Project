{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Gym Environment examples\n",
    "We can also take a look at the NN implementation in the url: https://github.com/stefanknegt/REPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import scipy.optimize as opt\n",
    "import pdb\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "print(env.observation_space.n)\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # print(env.observation_space.high)\n",
    "# # print(env.observation_space.low)\n",
    "# # print(env.observation_space)\n",
    "\n",
    "# # print(env.action_space.high)\n",
    "# # print(env.action_space.low)\n",
    "# # print(env.action_space)\n",
    "\n",
    "# env.n_states = 10*10*16\n",
    "# env.n_actions = 20\n",
    "\n",
    "# def discretize(env, n_observations, n_actions):\n",
    "#     L = []\n",
    "#     state_dim = env.observation_space.shape[0]\n",
    "#     action_dim = env.action_space.shape[0]\n",
    "#     for d in range(state_dim):\n",
    "#         L.append(np.linspace(env.observation_space.low[d], env.observation_space.high[d], n_observations[d]))\n",
    "#     for d in range(action_dim):\n",
    "#         L.append(np.linspace(env.action_space.low[d], env.action_space.high[d], n_actions[d]))\n",
    "#     res = np.meshgrid(*L)\n",
    "#     return res[0:len(n_observations)], res[len(n_observations):]\n",
    "\n",
    "# discrete_states, discrete_actions = discretize(env, [10,10,16], [20])\n",
    "# print(np.ravel(discrete_actions[0])[41])\n",
    "# def environment_check(name):\n",
    "#     '''\n",
    "#     This function checks the gym environment to get the action and state space.\n",
    "#     Also it returns the min and max value of the actions.\n",
    "#     '''\n",
    "#     env = gym.make(name)\n",
    "#     state_dim = env.observation_space.shape[0]\n",
    "#     action_dim = env.action_space.shape[0]\n",
    "#     action_min = float(env.action_space.low[0])\n",
    "#     action_max = float(env.action_space.high[0])\n",
    "\n",
    "#     return state_dim, action_dim, action_min, action_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy(object):\n",
    "    def __init__(self, pi):\n",
    "        n_states, n_actions = np.array(pi).shape()\n",
    "        self.n_actions = n_actions\n",
    "        self.n_states = n_states\n",
    "        self.pi = pi\n",
    "        \n",
    "    def draw_action(self,state):\n",
    "        u = np.random.rand()\n",
    "        probas = np.cumsum(self.pi[state,:])\n",
    "        a = 0\n",
    "        while (a < self.n_actions-1 and (u > probas[a] or self.pi[state,a]==0)):\n",
    "            a += 1\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_episodes(mdp, policy=None, horizon=None, n_episodes=1):\n",
    "    paths = []\n",
    "    for _ in range(n_episodes):\n",
    "        observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "\n",
    "        state = mdp.reset()\n",
    "        for _ in range(horizon):\n",
    "            action = policy.draw_action(state)\n",
    "            next_state, reward, terminal = mdp.step(action)\n",
    "            observations.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            state = copy.copy(next_state)\n",
    "            if terminal:\n",
    "                break\n",
    "        paths.append(dict(\n",
    "            states=np.array(observations),\n",
    "            actions=np.array(actions),\n",
    "            rewards=np.array(rewards),\n",
    "            next_states=np.array(next_states)\n",
    "        ))\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_phi(env, p):\n",
    "    \n",
    "    phi = np.zeros((env.observation_space.n,p))\n",
    "    \n",
    "    for k in range(env.observation_space.n):\n",
    "        phi[k,:] = [k,k**2,np.log(k+1)]\n",
    "    return(phi)\n",
    "    \n",
    "def initialize_pi(env):\n",
    "    pi = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for s in range(env.observation_space.n):\n",
    "        actions = np.linspace(-2, 2, 20)\n",
    "        for a in actions:\n",
    "            pi[s,a] = 1./len(actions)\n",
    "    print(pi)\n",
    "    return(pi)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_new_policy(eta, policy, phi, theta, samples):\n",
    "    log_new_pi = np.zeros((policy.n_states,policy.n_actions))\n",
    "    A = np.zeros((policy.n_states,policy.n_actions))\n",
    "    counter = np.zeros((policy.n_states,policy.n_actions))\n",
    "    nb_samples = 0\n",
    "    for i in range(len(samples)):\n",
    "        states = samples[i]['states']\n",
    "        actions = samples[i]['actions']\n",
    "        rewards = samples[i]['rewards']\n",
    "        next_states = samples[i]['next_states']\n",
    "\n",
    "        for j in range(len(states)):\n",
    "            A[states[j],actions[j]] += rewards[j] + np.dot(phi[next_states[j],:],theta) - np.dot(phi[states[j],:],theta)\n",
    "            counter[states[j],actions[j]] += 1\n",
    "            nb_samples += 1\n",
    "    for s in range(env.observation_space.n):\n",
    "        for a in range(env.action_space.n):\n",
    "            if counter[s,a]!=0:\n",
    "                A[s,a] /= counter[s,a]\n",
    "    for s in range(policy.n_states):\n",
    "        for a in range(policy.n_actions):\n",
    "            argexpo = np.zeros(policy.n_actions)\n",
    "            if policy.pi[s,a] == 0:\n",
    "                log_new_pi[s,a] = -float('inf')\n",
    "            else:\n",
    "                for b in range(policy.n_actions):\n",
    "                    argexpo[b] = np.log(policy.pi[s,b]+0.0001) + eta * A[s,b]\n",
    "                maxi = np.max(argexpo)\n",
    "                log_new_pi[s,a] = argexpo[a] - np.log(np.sum(np.exp(argexpo - maxi))) - maxi\n",
    "    print(np.exp(log_new_pi))\n",
    "    return(Policy(np.exp(log_new_pi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def g(theta, eta, phi, samples):\n",
    "    res = 0\n",
    "    A = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "    counter = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "    nb_samples = 0\n",
    "    for i in range(len(samples)):\n",
    "        states = samples[i]['states']\n",
    "        actions = samples[i]['actions']\n",
    "        rewards = samples[i]['rewards']\n",
    "        next_states = samples[i]['next_states']\n",
    "\n",
    "        for j in range(len(states)):\n",
    "            A[states[j],actions[j]] += rewards[j] + np.dot(phi[next_states[j],:],theta) - np.dot(phi[states[j],:],theta)\n",
    "            counter[states[j],actions[j]] += 1\n",
    "            nb_samples += 1\n",
    "    for s in range(env.observation_space.n):\n",
    "        for a in range(env.action_space.n):\n",
    "            if counter[s,a]!=0:\n",
    "                A[s,a] /= counter[s,a]\n",
    "    for i in range(len(samples)):\n",
    "        states = samples[i]['states']\n",
    "        actions = samples[i]['actions']\n",
    "        for j in range(len(states)):\n",
    "            res += np.exp(eta*A[states[j],actions[j]])\n",
    "    res /= nb_samples\n",
    "    return (np.log(res)/eta)\n",
    "\n",
    "def Dg(theta,eta,phi,samples):\n",
    "    n_states,p = np.shape(phi)\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    A = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "    D = np.zeros((env.observation_space.n,env.action_space.n,p))\n",
    "    counter = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "    for i in range(len(samples)):\n",
    "        states = samples[i]['states']\n",
    "        actions = samples[i]['actions']\n",
    "        rewards = samples[i]['rewards']\n",
    "        next_states = samples[i]['next_states']\n",
    "\n",
    "        for j in range(len(states)):\n",
    "            A[states[j],actions[j]] += rewards[j] + np.dot(phi[next_states[j],:],theta) - np.dot(phi[states[j],:],theta)\n",
    "            D[states[j],actions[j],:] += phi[next_states[j],:] - phi[states[j],:]\n",
    "            counter[states[j],actions[j]] += 1\n",
    "    for s in range(env.observation_space.n):\n",
    "        for a in range(env.action_space.n):\n",
    "            if counter[s,a]!=0:\n",
    "                A[s,a] /= counter[s,a]\n",
    "    for s in range(env.observation_space.n):\n",
    "        for a in range(env.action_space.n):\n",
    "            if counter[s,a]!=0:\n",
    "                D[s,a,:] /= counter[s,a]\n",
    "    for i in range(len(samples)):\n",
    "        states = samples[i]['states']\n",
    "        actions = samples[i]['actions']\n",
    "        for j in range(len(states)):\n",
    "            numerator += np.exp(eta*A[states[j],actions[j]]) * D[states[j],actions[j]]\n",
    "            denominator += np.exp(eta*A[states[j],actions[j]])\n",
    "    return ((1/eta) * numerator / denominator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def REPS_mirror_descent(env):\n",
    "    \"\"\"Relative Entropy Policy Search using Mirror Descent\"\"\"\n",
    "    p = 3    \n",
    "    # initialization of the distribution\n",
    "    pi = initialize_pi(env)\n",
    "    policy = Policy(pi)\n",
    "    #Tmax =  -100*np.log(10e-6)/(1-env.gamma)\n",
    "    K = 50\n",
    "    N = 100\n",
    "    eta = 0.1\n",
    "    theta = [0 for i in range(p)]\n",
    "    phi = compute_phi(env,p)\n",
    "    for k in range(K):\n",
    "        print('Iteration n°',k)\n",
    "        ##### SAMPLING\n",
    "        samples = collect_episodes(env, policy=policy, horizon=100, n_episodes=N)\n",
    "        \n",
    "        #### OPTIMIZE\n",
    "        theta = opt.fmin_bfgs(g,x0=theta,fprime=Dg,args=(eta,phi,samples))\n",
    "        \n",
    "        #### COMPUTE THE NEW POLICY\n",
    "        policy = compute_new_policy(eta,policy,phi,theta,samples)   \n",
    "    return(policy, theta, phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "[[ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]\n",
      " [ 0.05  0.05  0.05  0.05]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amine/miniconda3/lib/python3.5/site-packages/ipykernel/__main__.py:14: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-b6f05847fa66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mREPS_mirror_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-cbfa989556a1>\u001b[0m in \u001b[0;36mREPS_mirror_descent\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# initialization of the distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_pi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m#Tmax =  -100*np.log(10e-6)/(1-env.gamma)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-44aa839d9b81>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pi)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mn_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    for t in range(500):\n",
    "        if t%5==0:\n",
    "            env.render()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        policy = REPS_mirror_descent(env)[0]\n",
    "        action = policy.draw_action(observation)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning: A3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
